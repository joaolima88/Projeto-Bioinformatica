{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fWCcNW-wdZpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24b9c15-5b61-4910-921f-42887fbd7f19"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import joblib"
      ],
      "metadata": {
        "id": "bob8P0WAdWTi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Configurações de Ficheiros ---\n",
        "\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive\"\n",
        "\n",
        "TRAINING_DATA_FILE = os.path.join(BASE_DRIVE_PATH, \"training_data.csv\")\n",
        "DNABERT_EMBEDDINGS_FILE = os.path.join(BASE_DRIVE_PATH, \"dnabert_embeddings.npy\")\n",
        "NORMALIZATION_SCALER_FILE = os.path.join(BASE_DRIVE_PATH, \"scaler_standard.pkl\")\n",
        "\n",
        "RESULTS_OUTPUT_DIR = BASE_DRIVE_PATH\n",
        "os.makedirs(RESULTS_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "RESULTS_FILE_PREFIX = \"fnn_optimization_results\"\n",
        "PREDICTIONS_DATA_PREFIX = \"fnn_predictions_data\""
      ],
      "metadata": {
        "id": "9TnNdKd2dibq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Configurações da FNN ---\n",
        "FNN_HIDDEN_DIM = 256\n",
        "FNN_LEARNING_RATE = 0.001\n",
        "FNN_NUM_EPOCHS = 100\n",
        "FNN_BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "otuqB6HcdktJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Carregar Dados Originais e Embeddings Pré-Calculados ---\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(TRAINING_DATA_FILE)\n",
        "    print(f\"Dados originais carregados com sucesso de {TRAINING_DATA_FILE}. Dimensão: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: O ficheiro de dados original '{TRAINING_DATA_FILE}' não foi encontrado.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if not all(col in df.columns for col in ['target', 'sequence', 'prot_z']):\n",
        "    print(\"ERRO: Colunas inesperadas no DataFrame original\")\n",
        "    sys.exit(1)\n",
        "\n",
        "y_standardized = torch.tensor(df['prot_z'].values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "scaler = None\n",
        "try:\n",
        "    scaler = joblib.load(NORMALIZATION_SCALER_FILE)\n",
        "    print(f\"StandardScaler carregado com sucesso de: {NORMALIZATION_SCALER_FILE}\")\n",
        "    print(f\"Média do scaler para desestandardização: {scaler.mean_[0]:.4f}, Desvio Padrão: {scaler.scale_[0]:.4f}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: O ficheiro StandardScaler '{NORMALIZATION_SCALER_FILE}' não foi encontrado.\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"ERRO: Falha ao carregar StandardScaler de {NORMALIZATION_SCALER_FILE}: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "true_original_target_values = scaler.inverse_transform(df['prot_z'].values.reshape(-1, 1)).flatten()\n",
        "print(f\"Verdadeiros valores originais do target (para métricas) gerados. Exemplo (primeiros 5): {true_original_target_values[:5]}\")\n",
        "\n",
        "try:\n",
        "    X_embeddings = np.load(DNABERT_EMBEDDINGS_FILE)\n",
        "    X_embeddings = torch.tensor(X_embeddings, dtype=torch.float32)\n",
        "    if X_embeddings.shape[0] != len(df):\n",
        "        print(f\"AVISO: O número de amostras nos embeddings ({X_embeddings.shape[0]}) não corresponde ao número de amostras no CSV ({len(df)}).\")\n",
        "        sys.exit(1)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: O ficheiro de embeddings '{DNABERT_EMBEDDINGS_FILE}' não foi encontrado.\")\n",
        "    print(f\"Por favor, certifique-se de que o ficheiro .npy com os embeddings está na raiz do seu Google Drive (MyDrive) e acessível.\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"ERRO: Falha ao carregar o ficheiro de embeddings '{DNABERT_EMBEDDINGS_FILE}': {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"\\nDimensão de X (features): {X_embeddings.shape}\")\n",
        "print(f\"Dimensão de y (target 'prot_z' JÁ ESTANDARDIZADO): {y_standardized.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flmAiVepdnDh",
        "outputId": "c3c5af77-d86e-49b2-bc8b-1c0bb568d94b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados originais carregados com sucesso de /content/drive/MyDrive/training_data.csv. Dimensão: (9336, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StandardScaler carregado com sucesso de: /content/drive/MyDrive/scaler_standard.pkl\n",
            "Média do scaler para desestandardização: 61077.1584, Desvio Padrão: 64770.3892\n",
            "Verdadeiros valores originais do target (para métricas) gerados. Exemplo (primeiros 5): [1410.9918352  1443.35117936 1559.28121553 1501.84533491 1680.76889704]\n",
            "\n",
            "Dimensão de X (features): torch.Size([9336, 768])\n",
            "Dimensão de y (target 'prot_z' JÁ ESTANDARDIZADO): torch.Size([9336, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Feedforward Neural Network (FNN) ---\n",
        "class FNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
        "        super(FNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"FNN a treinar em dispositivo: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrM60a0DeCjp",
        "outputId": "57682314-cb26-462c-df5f-1bdade63807c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FNN a treinar em dispositivo: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Treinar e Avaliar a FNN com Validação Cruzada ---\n",
        "\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "mse_scores_standardized = []\n",
        "mae_scores_standardized = []\n",
        "r2_scores_standardized = []\n",
        "pearson_scores_standardized = []\n",
        "\n",
        "best_fnn_model_overall = None\n",
        "best_r2_val = -float('inf')\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X_embeddings)):\n",
        "    print(f\"\\n--- FNN - Fold {fold+1}/{n_splits} ---\")\n",
        "\n",
        "    X_train, X_test = X_embeddings[train_index], X_embeddings[test_index]\n",
        "    y_train, y_test = y_standardized[train_index], y_standardized[test_index]\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=FNN_BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=FNN_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    input_dim = X_embeddings.shape[1]\n",
        "    model = FNN(input_dim, FNN_HIDDEN_DIM).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=FNN_LEARNING_RATE)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(FNN_NUM_EPOCHS):\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 20 == 0 or epoch == 0 or epoch == FNN_NUM_EPOCHS - 1:\n",
        "            print(f'  Epoch [{epoch+1}/{FNN_NUM_EPOCHS}], Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    fold_predictions_standardized = []\n",
        "    fold_actual_standardized = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch_test, y_batch_test in test_loader:\n",
        "            X_batch_test, y_batch_test = X_batch_test.to(device), y_batch_test.to(device)\n",
        "            outputs_test = model(X_batch_test)\n",
        "            fold_predictions_standardized.extend(outputs_test.cpu().numpy().flatten())\n",
        "            fold_actual_standardized.extend(y_batch_test.cpu().numpy().flatten())\n",
        "\n",
        "    fold_predictions_standardized = np.array(fold_predictions_standardized)\n",
        "    fold_actual_standardized = np.array(fold_actual_standardized)\n",
        "\n",
        "    mse = mean_squared_error(fold_actual_standardized, fold_predictions_standardized)\n",
        "    mae = mean_absolute_error(fold_actual_standardized, fold_predictions_standardized)\n",
        "    r2 = r2_score(fold_actual_standardized, fold_predictions_standardized)\n",
        "\n",
        "    try:\n",
        "        pearson_corr, _ = pearsonr(fold_actual_standardized, fold_predictions_standardized)\n",
        "    except ValueError:\n",
        "        pearson_corr = np.nan\n",
        "\n",
        "    mse_scores_standardized.append(mse)\n",
        "    mae_scores_standardized.append(mae)\n",
        "    r2_scores_standardized.append(r2)\n",
        "    pearson_scores_standardized.append(pearson_corr)\n",
        "\n",
        "    print(f\"Fold {fold+1} Resultados (escala estandarizada): MSE={mse:.4f}, MAE={mae:.4f}, R2={r2:.4f}, Pearson={pearson_corr:.4f}\")\n",
        "\n",
        "    if r2 > best_r2_val:\n",
        "        best_r2_val = r2\n",
        "        best_fnn_model_overall = model.state_dict()\n",
        "\n",
        "print(\"\\n--- Resultados Finais da Validação Cruzada (Médias e Desvios Padrão) ---\")\n",
        "final_cv_results = {\n",
        "    \"MSE médio (escala estandarizada)\": f\"{np.nanmean(mse_scores_standardized):.4f} +/- {np.nanstd(mse_scores_standardized):.4f}\",\n",
        "    \"MAE médio (escala estandarizada)\": f\"{np.nanmean(mae_scores_standardized):.4f} +/- {np.nanstd(mae_scores_standardized):.4f}\",\n",
        "    \"R2 médio (escala estandarizada)\": f\"{np.nanmean(r2_scores_standardized):.4f} +/- {np.nanstd(r2_scores_standardized):.4f}\",\n",
        "    \"Correlação de Pearson média (escala estandarizada)\": f\"{np.nanmean(pearson_scores_standardized):.4f} +/- {np.nanstd(pearson_scores_standardized):.4f}\"\n",
        "}\n",
        "for metric, value in final_cv_results.items():\n",
        "    print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EchGP0LeF8o",
        "outputId": "f5576160-cdca-4f1a-98f3-93f69c877a72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FNN - Fold 1/5 ---\n",
            "  Epoch [1/100], Loss: 0.7750\n",
            "  Epoch [20/100], Loss: 0.2511\n",
            "  Epoch [40/100], Loss: 0.1284\n",
            "  Epoch [60/100], Loss: 0.0687\n",
            "  Epoch [80/100], Loss: 0.0354\n",
            "  Epoch [100/100], Loss: 0.0253\n",
            "Fold 1 Resultados (escala estandarizada): MSE=0.4469, MAE=0.4920, R2=0.5757, Pearson=0.7680\n",
            "\n",
            "--- FNN - Fold 2/5 ---\n",
            "  Epoch [1/100], Loss: 0.8120\n",
            "  Epoch [20/100], Loss: 0.2616\n",
            "  Epoch [40/100], Loss: 0.1281\n",
            "  Epoch [60/100], Loss: 0.0717\n",
            "  Epoch [80/100], Loss: 0.0394\n",
            "  Epoch [100/100], Loss: 0.0260\n",
            "Fold 2 Resultados (escala estandarizada): MSE=0.4480, MAE=0.4884, R2=0.5376, Pearson=0.7682\n",
            "\n",
            "--- FNN - Fold 3/5 ---\n",
            "  Epoch [1/100], Loss: 0.8110\n",
            "  Epoch [20/100], Loss: 0.2727\n",
            "  Epoch [40/100], Loss: 0.1483\n",
            "  Epoch [60/100], Loss: 0.0857\n",
            "  Epoch [80/100], Loss: 0.0502\n",
            "  Epoch [100/100], Loss: 0.0294\n",
            "Fold 3 Resultados (escala estandarizada): MSE=0.4303, MAE=0.4823, R2=0.5776, Pearson=0.7801\n",
            "\n",
            "--- FNN - Fold 4/5 ---\n",
            "  Epoch [1/100], Loss: 0.8006\n",
            "  Epoch [20/100], Loss: 0.2571\n",
            "  Epoch [40/100], Loss: 0.1331\n",
            "  Epoch [60/100], Loss: 0.0710\n",
            "  Epoch [80/100], Loss: 0.0385\n",
            "  Epoch [100/100], Loss: 0.0300\n",
            "Fold 4 Resultados (escala estandarizada): MSE=0.4158, MAE=0.4667, R2=0.5715, Pearson=0.7728\n",
            "\n",
            "--- FNN - Fold 5/5 ---\n",
            "  Epoch [1/100], Loss: 0.8035\n",
            "  Epoch [20/100], Loss: 0.2745\n",
            "  Epoch [40/100], Loss: 0.1393\n",
            "  Epoch [60/100], Loss: 0.0813\n",
            "  Epoch [80/100], Loss: 0.0417\n",
            "  Epoch [100/100], Loss: 0.0286\n",
            "Fold 5 Resultados (escala estandarizada): MSE=0.3933, MAE=0.4678, R2=0.6008, Pearson=0.7915\n",
            "\n",
            "--- Resultados Finais da Validação Cruzada (Médias e Desvios Padrão) ---\n",
            "MSE médio (escala estandarizada): 0.4268 +/- 0.0205\n",
            "MAE médio (escala estandarizada): 0.4794 +/- 0.0104\n",
            "R2 médio (escala estandarizada): 0.5727 +/- 0.0203\n",
            "Correlação de Pearson média (escala estandarizada): 0.7761 +/- 0.0089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Avaliação Final no Dataset Completo e Guardar Dados para Gráficos ---\n",
        "\n",
        "final_model = FNN(input_dim, FNN_HIDDEN_DIM).to(device)\n",
        "final_criterion = nn.MSELoss()\n",
        "final_optimizer = optim.Adam(final_model.parameters(), lr=FNN_LEARNING_RATE)\n",
        "\n",
        "full_dataset = TensorDataset(X_embeddings, y_standardized)\n",
        "full_loader = DataLoader(full_dataset, batch_size=FNN_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "final_model.train()\n",
        "for epoch in range(FNN_NUM_EPOCHS):\n",
        "    for X_batch, y_batch in full_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        final_optimizer.zero_grad()\n",
        "        outputs = final_model(X_batch)\n",
        "        loss = final_criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "\n",
        "final_model.eval()\n",
        "final_predictions_standardized = []\n",
        "with torch.no_grad():\n",
        "    for X_batch_test, _ in DataLoader(TensorDataset(X_embeddings, y_standardized), batch_size=FNN_BATCH_SIZE, shuffle=False):\n",
        "        X_batch_test = X_batch_test.to(device)\n",
        "        outputs_test = final_model(X_batch_test)\n",
        "        final_predictions_standardized.extend(outputs_test.cpu().numpy().flatten())\n",
        "\n",
        "final_predictions_standardized = np.array(final_predictions_standardized)\n",
        "\n",
        "final_predictions_original = scaler.inverse_transform(final_predictions_standardized.reshape(-1, 1)).flatten()\n",
        "\n",
        "global_mse = mean_squared_error(true_original_target_values, final_predictions_original)\n",
        "global_mae = mean_absolute_error(true_original_target_values, final_predictions_original)\n",
        "global_r2 = r2_score(true_original_target_values, final_predictions_original)\n",
        "try:\n",
        "    global_pearson, _ = pearsonr(true_original_target_values.flatten(), final_predictions_original.flatten())\n",
        "except ValueError:\n",
        "    global_pearson = np.nan\n",
        "\n",
        "global_results = {\n",
        "    \"MSE Global (escala original)\": f\"{global_mse:.4f}\",\n",
        "    \"MAE Global (escala original)\": f\"{global_mae:.4f}\",\n",
        "    \"R2 Global (escala original)\": f\"{global_r2:.4f}\",\n",
        "    \"Pearson Global (escala original)\": f\"{global_pearson:.4f}\"\n",
        "}\n",
        "for metric, value in global_results.items():\n",
        "    print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0RR04wjdHCE",
        "outputId": "26d79001-f246-4eff-ed4a-8bd6bd5524d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Global (escala original): 123223661.1736\n",
            "MAE Global (escala original): 8545.7397\n",
            "R2 Global (escala original): 0.9706\n",
            "Pearson Global (escala original): 0.9856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Guardar Resultados e Modelo ---\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_filename = os.path.join(RESULTS_OUTPUT_DIR, f\"{RESULTS_FILE_PREFIX}_standard_{timestamp}.txt\")\n",
        "predictions_data_filename = os.path.join(RESULTS_OUTPUT_DIR, f\"{PREDICTIONS_DATA_PREFIX}_standard_{timestamp}.npy\")\n",
        "model_save_filename = os.path.join(RESULTS_OUTPUT_DIR, f\"best_standard_fnn_model_{timestamp}.pth\")\n",
        "\n",
        "print(f\"\\n--- A guardar resultados em: {results_filename} ---\")\n",
        "with open(results_filename, 'w') as f:\n",
        "    f.write(f\"Resultados da Otimização da FNN\\n\")\n",
        "    f.write(f\"Data e Hora: {timestamp}\\n\")\n",
        "    f.write(f\"----------------------------------------------------\\n\\n\")\n",
        "\n",
        "    f.write(f\"Hiperparâmetros da FNN:\\n\")\n",
        "    f.write(f\"  Dimensão Oculta: {FNN_HIDDEN_DIM}\\n\")\n",
        "    f.write(f\"  Taxa de Aprendizagem: {FNN_LEARNING_RATE}\\n\")\n",
        "    f.write(f\"  Número de Épocas: {FNN_NUM_EPOCHS}\\n\")\n",
        "    f.write(f\"  Tamanho do Lote: {FNN_BATCH_SIZE}\\n\")\n",
        "    f.write(f\"\\n\")\n",
        "\n",
        "    f.write(f\"Resultados da Validação Cruzada (Médias e Desvios Padrão - ESCALA ESTANDARDIZADA):\\n\")\n",
        "    for metric, value in final_cv_results.items():\n",
        "        f.write(f\"  {metric}: {value}\\n\")\n",
        "    f.write(f\"\\n\")\n",
        "\n",
        "    f.write(f\"Métricas no Dataset Completo (Com o modelo final treinado uma vez no dataset inteiro - ESCALA ORIGINAL):\\n\")\n",
        "    for metric, value in global_results.items():\n",
        "        f.write(f\"  {metric}: {value}\\n\")\n",
        "    f.write(f\"\\n\")\n",
        "    f.write(f\"Informações Adicionais:\\n\")\n",
        "    f.write(f\"  Número de Amostras: {X_embeddings.shape[0]}\\n\")\n",
        "    f.write(f\"  Número de Features: {X_embeddings.shape[1]}\\n\")\n",
        "    f.write(f\"  Tipo de Embeddings: DNABERT\\n\")\n",
        "    f.write(f\"  Folds de Validação Cruzada: {n_splits}\\n\")\n",
        "    f.write(f\"  Ficheiro de Embeddings Usado: {os.path.basename(DNABERT_EMBEDDINGS_FILE)}\\n\")\n",
        "    f.write(f\"  Ficheiro de Modelo FNN Guardado: {os.path.basename(model_save_filename)}\\n\")\n",
        "    f.write(f\"  Ficheiro de Dados de Previsões Guardado: {os.path.basename(predictions_data_filename)}\\n\")\n",
        "    f.write(f\"  Ficheiro de StandardScaler Guardado: {os.path.basename(NORMALIZATION_SCALER_FILE)}\\n\")\n",
        "\n",
        "\n",
        "print(f\"Resultados guardados com sucesso em {results_filename}.\")\n",
        "\n",
        "data_for_plotting = np.vstack((true_original_target_values.flatten(), final_predictions_original)).T\n",
        "np.save(predictions_data_filename, data_for_plotting)\n",
        "print(f\"Valores reais (escala original) e previstos (escala original) guardados em {predictions_data_filename} para análise de gráficos.\")\n",
        "\n",
        "torch.save(final_model.state_dict(), model_save_filename)\n",
        "print(f\"Melhor modelo FNN guardado em: {model_save_filename}.\")\n",
        "\n",
        "print(\"\\nScript de treino e avaliação da FNN concluído.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on59mmgWQkVc",
        "outputId": "8d5d83e0-72f4-4d0b-a5d8-e7b627d744fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- A guardar resultados em: /content/drive/MyDrive/fnn_optimization_results_standard_20250620_230810.txt ---\n",
            "Resultados guardados com sucesso em /content/drive/MyDrive/fnn_optimization_results_standard_20250620_230810.txt.\n",
            "Valores reais (escala original) e previstos (escala original) guardados em /content/drive/MyDrive/fnn_predictions_data_standard_20250620_230810.npy para análise de gráficos.\n",
            "Melhor modelo FNN guardado em: /content/drive/MyDrive/best_standard_fnn_model_20250620_230810.pth.\n",
            "\n",
            "Script de treino e avaliação da FNN concluído.\n"
          ]
        }
      ]
    }
  ]
}