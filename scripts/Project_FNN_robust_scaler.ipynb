{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhZDL9mL6E_X",
        "outputId": "91fc6729-11ea-4c89-8a8e-22b34b24275c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import RobustScaler # ALTERADO: De StandardScaler para RobustScaler\n",
        "import os\n",
        "import sys\n",
        "import datetime\n",
        "import joblib"
      ],
      "metadata": {
        "id": "wyc7Uum26MoP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive\"\n",
        "\n",
        "TRAINING_DATA_FILE = os.path.join(BASE_DRIVE_PATH, \"training_data.csv\")\n",
        "\n",
        "DNABERT_EMBEDDINGS_FILE = os.path.join(BASE_DRIVE_PATH, \"dnabert_embeddings.npy\")\n",
        "\n",
        "NORMALIZATION_SCALER_FILE = os.path.join(BASE_DRIVE_PATH, \"scaler_robust.pkl\")\n",
        "\n",
        "RESULTS_OUTPUT_DIR = BASE_DRIVE_PATH\n",
        "os.makedirs(RESULTS_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "RESULTS_FILE_PREFIX = \"fnn_robust_results\"\n",
        "PREDICTIONS_DATA_PREFIX = \"fnn_robust_predictions_data\""
      ],
      "metadata": {
        "id": "MojKO6Gf6Mln"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Configurações da FNN ---\n",
        "FNN_HIDDEN_DIM = 256\n",
        "FNN_LEARNING_RATE = 0.001\n",
        "FNN_NUM_EPOCHS = 100\n",
        "FNN_BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "tosHWZwA6Mc_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Carregar Dados Originais e Embeddings Pré-Calculados ---\n",
        "print(\"\\n--- A carregar dados e embeddings pré-calculados ---\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(TRAINING_DATA_FILE)\n",
        "    print(f\"Dados originais carregados com sucesso de {TRAINING_DATA_FILE}. Dimensão: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: O ficheiro de dados original '{TRAINING_DATA_FILE}' não foi encontrado.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if not all(col in df.columns for col in ['target', 'sequence', 'prot_scaled']):\n",
        "    print(\"ERRO: Colunas inesperadas no DataFrame original\")\n",
        "    sys.exit(1)\n",
        "\n",
        "y_scaled = torch.tensor(df['prot_scaled'].values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# CARREGAR O SCALER EXISTENTE\n",
        "scaler = None\n",
        "try:\n",
        "\n",
        "    scaler = joblib.load(NORMALIZATION_SCALER_FILE)\n",
        "    print(f\"RobustScaler carregado com sucesso de: {NORMALIZATION_SCALER_FILE}\")\n",
        "    print(f\"Centro do scaler para desescalar: {scaler.center_[0]:.4f}, Escala: {scaler.scale_[0]:.4f}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: O ficheiro RobustScaler '{NORMALIZATION_SCALER_FILE}' não foi encontrado.\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"ERRO: Falha ao carregar RobustScaler de {NORMALIZATION_SCALER_FILE}: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "true_original_target_values = scaler.inverse_transform(df['prot_scaled'].values.reshape(-1, 1)).flatten()\n",
        "\n",
        "try:\n",
        "    X_embeddings = np.load(DNABERT_EMBEDDINGS_FILE)\n",
        "    X_embeddings = torch.tensor(X_embeddings, dtype=torch.float32)\n",
        "    if X_embeddings.shape[0] != len(df):\n",
        "        print(f\"AVISO: O número de amostras nos embeddings ({X_embeddings.shape[0]}) não corresponde ao número de amostras no CSV ({len(df)}).\")\n",
        "    print(f\"Embeddings DNABERT carregados com sucesso de {DNABERT_EMBEDDINGS_FILE}. Dimensão: {X_embeddings.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: O ficheiro de embeddings '{DNABERT_EMBEDDINGS_FILE}' não foi encontrado.\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"ERRO: Falha ao carregar o ficheiro de embeddings '{DNABERT_EMBEDDINGS_FILE}': {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"\\nDimensão de X (features): {X_embeddings.shape}\")\n",
        "print(f\"Dimensão de y (target 'prot_scaled' JÁ ESCALADO): {y_scaled.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNciwNc96Maf",
        "outputId": "9e42356f-e7e3-4470-ec6a-37911c755e96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- A carregar dados e embeddings pré-calculados ---\n",
            "Dados originais carregados com sucesso de /content/drive/MyDrive/training_data.csv. Dimensão: (9336, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RobustScaler from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RobustScaler carregado com sucesso de: /content/drive/MyDrive/scaler_robust.pkl\n",
            "Centro do scaler para desescalar: 32619.6512, Escala: 93883.9232\n",
            "Embeddings DNABERT carregados com sucesso de /content/drive/MyDrive/dnabert_embeddings.npy. Dimensão: torch.Size([9336, 768])\n",
            "\n",
            "Dimensão de X (features): torch.Size([9336, 768])\n",
            "Dimensão de y (target 'prot_scaled' JÁ ESCALADO): torch.Size([9336, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Definição da Feedforward Neural Network (FNN) ---\n",
        "class FNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
        "        super(FNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"FNN a treinar em dispositivo: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J49XvOzn6hW-",
        "outputId": "c0064781-6e87-4cf8-a1e7-778310238cb2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FNN a treinar em dispositivo: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Treinar e Avaliar a FNN com Validação Cruzada ---\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "mse_scores_scaled = [] # ALTERADO\n",
        "mae_scores_scaled = [] # ALTERADO\n",
        "r2_scores_scaled = [] # ALTERADO\n",
        "pearson_scores_scaled = [] # ALTERADO\n",
        "\n",
        "best_fnn_model_overall = None\n",
        "best_r2_val = -float('inf')\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X_embeddings)):\n",
        "    print(f\"\\n--- FNN - Fold {fold+1}/{n_splits} ---\")\n",
        "\n",
        "    X_train, X_test = X_embeddings[train_index], X_embeddings[test_index]\n",
        "    y_train, y_test = y_scaled[train_index], y_scaled[test_index] # y_test é escalado\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=FNN_BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=FNN_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    input_dim = X_embeddings.shape[1]\n",
        "    model = FNN(input_dim, FNN_HIDDEN_DIM).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=FNN_LEARNING_RATE)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(FNN_NUM_EPOCHS):\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 20 == 0 or epoch == 0 or epoch == FNN_NUM_EPOCHS - 1:\n",
        "            print(f'  Epoch [{epoch+1}/{FNN_NUM_EPOCHS}], Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    fold_predictions_scaled = []\n",
        "    fold_actual_scaled = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch_test, y_batch_test in test_loader:\n",
        "            X_batch_test, y_batch_test = X_batch_test.to(device), y_batch_test.to(device)\n",
        "            outputs_test = model(X_batch_test)\n",
        "            fold_predictions_scaled.extend(outputs_test.cpu().numpy().flatten())\n",
        "            fold_actual_scaled.extend(y_batch_test.cpu().numpy().flatten())\n",
        "\n",
        "    fold_predictions_scaled = np.array(fold_predictions_scaled)\n",
        "    fold_actual_scaled = np.array(fold_actual_scaled)\n",
        "    mse = mean_squared_error(fold_actual_scaled, fold_predictions_scaled)\n",
        "    mae = mean_absolute_error(fold_actual_scaled, fold_predictions_scaled)\n",
        "    r2 = r2_score(fold_actual_scaled, fold_predictions_scaled)\n",
        "\n",
        "    try:\n",
        "        pearson_corr, _ = pearsonr(fold_actual_scaled, fold_predictions_scaled)\n",
        "    except ValueError:\n",
        "        pearson_corr = np.nan\n",
        "\n",
        "    mse_scores_scaled.append(mse)\n",
        "    mae_scores_scaled.append(mae)\n",
        "    r2_scores_scaled.append(r2)\n",
        "    pearson_scores_scaled.append(pearson_corr)\n",
        "\n",
        "    print(f\"Fold {fold+1} Resultados (escala escalada): MSE={mse:.4f}, MAE={mae:.4f}, R2={r2:.4f}, Pearson={pearson_corr:.4f}\")\n",
        "\n",
        "    if r2 > best_r2_val:\n",
        "        best_r2_val = r2\n",
        "        best_fnn_model_overall = model.state_dict()\n",
        "\n",
        "print(\"\\n--- Resultados Finais da Validação Cruzada (Médias e Desvios Padrão) ---\")\n",
        "final_cv_results = {\n",
        "    \"MSE médio (escala escalada)\": f\"{np.nanmean(mse_scores_scaled):.4f} +/- {np.nanstd(mse_scores_scaled):.4f}\",\n",
        "    \"MAE médio (escala escalada)\": f\"{np.nanmean(mae_scores_scaled):.4f} +/- {np.nanstd(mae_scores_scaled):.4f}\",\n",
        "    \"R2 médio (escala escalada)\": f\"{np.nanmean(r2_scores_scaled):.4f} +/- {np.nanstd(r2_scores_scaled):.4f}\",\n",
        "    \"Correlação de Pearson média (escala escalada)\": f\"{np.nanmean(pearson_scores_scaled):.4f} +/- {np.nanstd(pearson_scores_scaled):.4f}\"\n",
        "}\n",
        "for metric, value in final_cv_results.items():\n",
        "    print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_j-SPLN6hVf",
        "outputId": "ac78c9b3-e2ff-43e6-9800-e51fb30bfe8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- FNN - Fold 1/5 ---\n",
            "  Epoch [1/100], Loss: 0.3678\n",
            "  Epoch [20/100], Loss: 0.1132\n",
            "  Epoch [40/100], Loss: 0.0586\n",
            "  Epoch [60/100], Loss: 0.0285\n",
            "  Epoch [80/100], Loss: 0.0159\n",
            "  Epoch [100/100], Loss: 0.0109\n",
            "Fold 1 Resultados (escala escalada): MSE=0.2150, MAE=0.3370, R2=0.5711, Pearson=0.7712\n",
            "\n",
            "--- FNN - Fold 2/5 ---\n",
            "  Epoch [1/100], Loss: 0.3790\n",
            "  Epoch [20/100], Loss: 0.1236\n",
            "  Epoch [40/100], Loss: 0.0667\n",
            "  Epoch [60/100], Loss: 0.0337\n",
            "  Epoch [80/100], Loss: 0.0200\n",
            "  Epoch [100/100], Loss: 0.0111\n",
            "Fold 2 Resultados (escala escalada): MSE=0.1964, MAE=0.3186, R2=0.5740, Pearson=0.7762\n",
            "\n",
            "--- FNN - Fold 3/5 ---\n",
            "  Epoch [1/100], Loss: 0.3731\n",
            "  Epoch [20/100], Loss: 0.1230\n",
            "  Epoch [40/100], Loss: 0.0626\n",
            "  Epoch [60/100], Loss: 0.0344\n",
            "  Epoch [80/100], Loss: 0.0237\n",
            "  Epoch [100/100], Loss: 0.0122\n",
            "Fold 3 Resultados (escala escalada): MSE=0.1977, MAE=0.3257, R2=0.5924, Pearson=0.7804\n",
            "\n",
            "--- FNN - Fold 4/5 ---\n",
            "  Epoch [1/100], Loss: 0.3782\n",
            "  Epoch [20/100], Loss: 0.1148\n",
            "  Epoch [40/100], Loss: 0.0621\n",
            "  Epoch [60/100], Loss: 0.0365\n",
            "  Epoch [80/100], Loss: 0.0192\n",
            "  Epoch [100/100], Loss: 0.0148\n",
            "Fold 4 Resultados (escala escalada): MSE=0.2004, MAE=0.3200, R2=0.5660, Pearson=0.7723\n",
            "\n",
            "--- FNN - Fold 5/5 ---\n",
            "  Epoch [1/100], Loss: 0.3854\n",
            "  Epoch [20/100], Loss: 0.1265\n",
            "  Epoch [40/100], Loss: 0.0650\n",
            "  Epoch [60/100], Loss: 0.0358\n",
            "  Epoch [80/100], Loss: 0.0228\n",
            "  Epoch [100/100], Loss: 0.0181\n",
            "Fold 5 Resultados (escala escalada): MSE=0.1963, MAE=0.3212, R2=0.5813, Pearson=0.7705\n",
            "\n",
            "--- Resultados Finais da Validação Cruzada (Médias e Desvios Padrão) ---\n",
            "MSE médio (escala escalada): 0.2012 +/- 0.0071\n",
            "MAE médio (escala escalada): 0.3245 +/- 0.0067\n",
            "R2 médio (escala escalada): 0.5770 +/- 0.0091\n",
            "Correlação de Pearson média (escala escalada): 0.7741 +/- 0.0037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Avaliação Final no Dataset Completo e Guardar Dados para Gráficos ---\n",
        "\n",
        "final_model = FNN(input_dim, FNN_HIDDEN_DIM).to(device)\n",
        "final_criterion = nn.MSELoss()\n",
        "final_optimizer = optim.Adam(final_model.parameters(), lr=FNN_LEARNING_RATE)\n",
        "\n",
        "full_dataset = TensorDataset(X_embeddings, y_scaled)\n",
        "full_loader = DataLoader(full_dataset, batch_size=FNN_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "final_model.train()\n",
        "for epoch in range(FNN_NUM_EPOCHS):\n",
        "    for X_batch, y_batch in full_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        final_optimizer.zero_grad()\n",
        "        outputs = final_model(X_batch)\n",
        "        loss = final_criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "\n",
        "final_model.eval()\n",
        "final_predictions_scaled = []\n",
        "with torch.no_grad():\n",
        "    for X_batch_test, _ in DataLoader(TensorDataset(X_embeddings, y_scaled), batch_size=FNN_BATCH_SIZE, shuffle=False):\n",
        "        X_batch_test = X_batch_test.to(device)\n",
        "        outputs_test = final_model(X_batch_test)\n",
        "        final_predictions_scaled.extend(outputs_test.cpu().numpy().flatten())\n",
        "\n",
        "final_predictions_scaled = np.array(final_predictions_scaled)\n",
        "\n",
        "# --- Desescalar as previsões para obter valores reais ---\n",
        "# usando inverse_transform do RobustScaler\n",
        "final_predictions_original = scaler.inverse_transform(final_predictions_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "global_mse = mean_squared_error(true_original_target_values, final_predictions_original)\n",
        "global_mae = mean_absolute_error(true_original_target_values, final_predictions_original)\n",
        "global_r2 = r2_score(true_original_target_values, final_predictions_original)\n",
        "try:\n",
        "    pearson_corr_global, _ = pearsonr(true_original_target_values.flatten(), final_predictions_original.flatten())\n",
        "except ValueError:\n",
        "    pearson_corr_global = np.nan\n",
        "\n",
        "global_results = {\n",
        "    \"MSE Global (escala original)\": f\"{global_mse:.4f}\",\n",
        "    \"MAE Global (escala original)\": f\"{global_mae:.4f}\",\n",
        "    \"R2 Global (escala original)\": f\"{global_r2:.4f}\",\n",
        "    \"Pearson Global (escala original)\": f\"{pearson_corr_global:.4f}\"\n",
        "}\n",
        "for metric, value in global_results.items():\n",
        "    print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK60bJ4x6hSw",
        "outputId": "3a3eb86e-77af-4149-b7b6-5ed99ff62890"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Global (escala original): 111427405.1398\n",
            "MAE Global (escala original): 8107.8810\n",
            "R2 Global (escala original): 0.9734\n",
            "Pearson Global (escala original): 0.9867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7P0fIuv95s0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496f03e9-158b-4468-eaea-5c41a8a1cbb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- A guardar resultados em: /content/drive/MyDrive/fnn_robust_results_20250620_224238.txt ---\n",
            "Resultados guardados com sucesso em /content/drive/MyDrive/fnn_robust_results_20250620_224238.txt.\n",
            "Valores reais (escala original) e previstos (escala original) guardados em /content/drive/MyDrive/fnn_robust_predictions_data_20250620_224238.npy para análise de gráficos.\n",
            "Melhor modelo FNN guardado em: /content/drive/MyDrive/best_fnn_model_robust_20250620_224238.pth.\n",
            "\n",
            "Script de treino e avaliação da FNN concluído.\n"
          ]
        }
      ],
      "source": [
        "# --- 7. Guardar Resultados e Modelo ---\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_filename = os.path.join(RESULTS_OUTPUT_DIR, f\"{RESULTS_FILE_PREFIX}_{timestamp}.txt\")\n",
        "predictions_data_filename = os.path.join(RESULTS_OUTPUT_DIR, f\"{PREDICTIONS_DATA_PREFIX}_{timestamp}.npy\")\n",
        "model_save_filename = os.path.join(RESULTS_OUTPUT_DIR, f\"best_fnn_model_robust_{timestamp}.pth\") # ALTERADO: Nome para refletir RobustScaler\n",
        "\n",
        "print(f\"\\n--- A guardar resultados em: {results_filename} ---\")\n",
        "with open(results_filename, 'w') as f:\n",
        "    f.write(f\"Resultados da Otimização da FNN com RobustScaler\\n\") # ALTERADO\n",
        "    f.write(f\"Data e Hora: {timestamp}\\n\")\n",
        "    f.write(f\"----------------------------------------------------\\n\\n\")\n",
        "\n",
        "    f.write(f\"Hiperparâmetros da FNN:\\n\")\n",
        "    f.write(f\"  Dimensão Oculta: {FNN_HIDDEN_DIM}\\n\")\n",
        "    f.write(f\"  Taxa de Aprendizagem: {FNN_LEARNING_RATE}\\n\")\n",
        "    f.write(f\"  Número de Épocas: {FNN_NUM_EPOCHS}\\n\")\n",
        "    f.write(f\"  Tamanho do Lote: {FNN_BATCH_SIZE}\\n\")\n",
        "    f.write(f\"\\n\")\n",
        "\n",
        "    f.write(f\"Resultados da Validação Cruzada (Médias e Desvios Padrão - ESCALA ESCALADA (RobustScaler)):\\n\") # ALTERADO\n",
        "    for metric, value in final_cv_results.items():\n",
        "        f.write(f\"  {metric}: {value}\\n\")\n",
        "    f.write(f\"\\n\")\n",
        "\n",
        "    f.write(f\"Métricas no Dataset Completo (Com o modelo final treinado uma vez no dataset inteiro - ESCALA ORIGINAL):\\n\")\n",
        "    for metric, value in global_results.items():\n",
        "        f.write(f\"  {metric}: {value}\\n\")\n",
        "    f.write(f\"\\n\")\n",
        "    f.write(f\"Informações Adicionais:\\n\")\n",
        "    f.write(f\"  Número de Amostras: {X_embeddings.shape[0]}\\n\")\n",
        "    f.write(f\"  Número de Features: {X_embeddings.shape[1]}\\n\")\n",
        "    f.write(f\"  Tipo de Embeddings: DNABERT\\n\")\n",
        "    f.write(f\"  Folds de Validação Cruzada: {n_splits}\\n\")\n",
        "    f.write(f\"  Ficheiro de Embeddings Usado: {os.path.basename(DNABERT_EMBEDDINGS_FILE)}\\n\")\n",
        "    f.write(f\"  Ficheiro de Modelo FNN Guardado: {os.path.basename(model_save_filename)}\\n\")\n",
        "    f.write(f\"  Ficheiro de Dados de Previsões Guardado: {os.path.basename(predictions_data_filename)}\\n\")\n",
        "    f.write(f\"  Ficheiro do Scaler Guardado: {os.path.basename(NORMALIZATION_SCALER_FILE)}\\n\") # ALTERADO\n",
        "\n",
        "\n",
        "print(f\"Resultados guardados com sucesso em {results_filename}.\")\n",
        "\n",
        "data_for_plotting = np.vstack((true_original_target_values.flatten(), final_predictions_original)).T # ALTERADO\n",
        "np.save(predictions_data_filename, data_for_plotting)\n",
        "print(f\"Valores reais (escala original) e previstos (escala original) guardados em {predictions_data_filename} para análise de gráficos.\")\n",
        "\n",
        "torch.save(final_model.state_dict(), model_save_filename)\n",
        "print(f\"Melhor modelo FNN guardado em: {model_save_filename}.\")\n",
        "\n",
        "print(\"\\nScript de treino e avaliação da FNN concluído.\")"
      ]
    }
  ]
}