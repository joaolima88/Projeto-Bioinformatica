{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# --- 0. Importações de Bibliotecas Essenciais ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt # NOVO: Importar matplotlib para plotagem"
   ],
   "metadata": {
    "id": "wyc7Uum26MoP",
    "ExecuteTime": {
     "end_time": "2025-06-20T22:48:35.227389Z",
     "start_time": "2025-06-20T22:48:34.966187Z"
    }
   },
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 1. Configurações de Ficheiros ---\n",
    "\n",
    "BASE_PATH = \".\" # O diretório atual onde este script está a ser executado\n",
    "\n",
    "TRAINING_DATA_FILE = os.path.join(BASE_PATH, \"training_data.csv\")\n",
    "DNABERT_EMBEDDINGS_FILE = os.path.join(BASE_PATH, \"dnabert_embeddings.npy\")\n",
    "NORMALIZATION_SCALER_FILE = os.path.join(BASE_PATH, \"scaler_robust.pkl\")\n",
    "\n",
    "RESULTS_OUTPUT_DIR = BASE_PATH\n",
    "os.makedirs(RESULTS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "RESULTS_FILE_PREFIX = \"fnn_optimization_results\"\n",
    "PREDICTIONS_DATA_PREFIX = \"fnn_predictions_data\""
   ],
   "metadata": {
    "id": "MojKO6Gf6Mln",
    "ExecuteTime": {
     "end_time": "2025-06-20T22:48:56.215639Z",
     "start_time": "2025-06-20T22:48:56.205818Z"
    }
   },
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T22:49:24.311744Z",
     "start_time": "2025-06-20T22:49:24.286135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2. Configurações da FNN e Novas Configurações de Treino ---\n",
    "FNN_HIDDEN_DIM_1 = 256\n",
    "FNN_HIDDEN_DIM_2 = 128\n",
    "FNN_LEARNING_RATE = 0.001\n",
    "FNN_NUM_EPOCHS = 250\n",
    "FNN_BATCH_SIZE = 64\n",
    "\n",
    "FNN_DROPOUT_RATE = 0.2\n",
    "FNN_WEIGHT_DECAY = 1e-5\n",
    "EARLY_STOPPING_PATIENCE = 25\n",
    "\n",
    "LR_SCHEDULER_FACTOR = 0.5\n",
    "LR_SCHEDULER_PATIENCE = 10\n",
    "LR_SCHEDULER_MIN_LR = 1e-6"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "source": [
    "# --- FUNÇÃO AUXILIAR ---\n",
    "def plot_loss_curves(train_losses, val_losses, fold_num=None, title_suffix=\"\", save_dir=None, prefix=\"loss_curve\"):\n",
    "    \"\"\"\n",
    "    Cria as curvas de perda de treino e validação.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Perda de Treino')\n",
    "\n",
    "    if val_losses:\n",
    "        plt.plot(val_losses, label='Perda de Validação')\n",
    "\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Perda (MSE)')\n",
    "\n",
    "    if fold_num is not None:\n",
    "        plt.title(f'Curvas de Perda da FNN - Fold {fold_num} {title_suffix}')\n",
    "    else:\n",
    "        plt.title(f'Curvas de Perda da FNN - {title_suffix}')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_dir:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if fold_num is not None:\n",
    "            filename = os.path.join(save_dir, f\"{prefix}_fold_{fold_num}_{timestamp}.png\")\n",
    "        else:\n",
    "            filename = os.path.join(save_dir, f\"{prefix}_{timestamp}.png\")\n",
    "        plt.savefig(filename)\n",
    "        print(f\"Curva de perda salva em: {filename}\")\n",
    "    plt.close()"
   ],
   "metadata": {
    "id": "tosHWZwA6Mc_",
    "ExecuteTime": {
     "end_time": "2025-06-20T22:49:52.570670Z",
     "start_time": "2025-06-20T22:49:52.561600Z"
    }
   },
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 3. Carregar Dados Originais e Embeddings Pré-Calculados ---\n",
    "print(\"\\n--- A carregar dados e embeddings pré-calculados ---\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(TRAINING_DATA_FILE)\n",
    "    print(f\"Dados originais carregados com sucesso de {TRAINING_DATA_FILE}. Dimensão: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: O ficheiro de dados original '{TRAINING_DATA_FILE}' não foi encontrado.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not all(col in df.columns for col in ['target', 'sequence', 'prot_scaled']):\n",
    "    print(\"ERRO: Colunas inesperadas no DataFrame original.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "y_scaled = torch.tensor(df['prot_scaled'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "scaler = None\n",
    "try:\n",
    "    scaler = joblib.load(NORMALIZATION_SCALER_FILE)\n",
    "    print(f\"RobustScaler carregado com sucesso de: {NORMALIZATION_SCALER_FILE}\")\n",
    "    print(f\"Centro do scaler para desescalar: {scaler.center_[0]:.4f}, Escala: {scaler.scale_[0]:.4f}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: O ficheiro RobustScaler '{NORMALIZATION_SCALER_FILE}' não foi encontrado.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Falha ao carregar RobustScaler de {NORMALIZATION_SCALER_FILE}: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "true_original_target_values = scaler.inverse_transform(df['prot_scaled'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "try:\n",
    "    X_embeddings = np.load(DNABERT_EMBEDDINGS_FILE)\n",
    "    X_embeddings = torch.tensor(X_embeddings, dtype=torch.float32)\n",
    "    if X_embeddings.shape[0] != len(df):\n",
    "        print(f\"AVISO: O número de amostras nos embeddings ({X_embeddings.shape[0]}) não corresponde ao número de amostras no CSV ({len(df)}).\")\n",
    "        sys.exit(1)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: O ficheiro de embeddings '{DNABERT_EMBEDDINGS_FILE}' não foi encontrado.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Falha ao carregar o ficheiro de embeddings '{DNABERT_EMBEDDINGS_FILE}': {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\nDimensão de X (features): {X_embeddings.shape}\")\n",
    "print(f\"Dimensão de y (target 'prot_scaled' JÁ ESCALADO): {y_scaled.shape}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNciwNc96Maf",
    "outputId": "207b0fa3-6d4f-4571-de1e-2e790fc1f609",
    "ExecuteTime": {
     "end_time": "2025-06-20T18:35:38.570384Z",
     "start_time": "2025-06-20T18:35:38.316903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- A carregar dados e embeddings pré-calculados ---\n",
      "Dados originais carregados com sucesso de .\\training_data.csv. Dimensão: (9336, 4)\n",
      "RobustScaler carregado com sucesso de: .\\scaler_robust.pkl\n",
      "Centro do scaler para desescalar: 32619.6512, Escala: 93883.9232\n",
      "Verdadeiros valores originais do target (para métricas) gerados. Exemplo (primeiros 5): [1410.99 1443.35 1559.28 1501.85 1680.77]\n",
      "Embeddings DNABERT carregados com sucesso de .\\dnabert_embeddings.npy. Dimensão: torch.Size([9336, 768])\n",
      "\n",
      "Dimensão de X (features): torch.Size([9336, 768])\n",
      "Dimensão de y (target 'prot_scaled' JÁ ESCALADO): torch.Size([9336, 1])\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 4. Definição da Feedforward Neural Network (FNN) ---\n",
    "class FNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, output_dim=1, dropout_rate=0.0):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim_1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate) # Dropout após a primeira camada\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim_1, hidden_dim_2) # Segunda camada oculta\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate) # Dropout após a segunda camada\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_dim_2, output_dim) # Camada de saída\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x) # Passagem pela segunda camada\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"dispositivo: {device}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J49XvOzn6hW-",
    "outputId": "4d98aac8-6c4a-4e3a-a9e5-f4a4a5a4fd3e",
    "ExecuteTime": {
     "end_time": "2025-06-20T18:35:41.938399Z",
     "start_time": "2025-06-20T18:35:41.915642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN a treinar em dispositivo: cpu\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 5. Treinar e Avaliar a FNN com Validação Cruzada ---\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "mse_scores_scaled = []\n",
    "mae_scores_scaled = []\n",
    "r2_scores_scaled = []\n",
    "pearson_scores_scaled = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_embeddings)):\n",
    "    print(f\"\\n--- FNN - Fold {fold+1}/{n_splits} ---\")\n",
    "\n",
    "    X_train, X_test = X_embeddings[train_index], X_embeddings[test_index]\n",
    "    y_train, y_test = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=FNN_BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=FNN_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    input_dim = X_embeddings.shape[1]\n",
    "    model = FNN(input_dim, FNN_HIDDEN_DIM_1, FNN_HIDDEN_DIM_2, dropout_rate=FNN_DROPOUT_RATE).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=FNN_LEARNING_RATE, weight_decay=FNN_WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=LR_SCHEDULER_FACTOR,\n",
    "        patience=LR_SCHEDULER_PATIENCE,\n",
    "        min_lr=LR_SCHEDULER_MIN_LR,\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_epoch_model_state = None\n",
    "\n",
    "    train_losses_fold = []\n",
    "    val_losses_fold = []\n",
    "\n",
    "    for epoch in range(FNN_NUM_EPOCHS):\n",
    "\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # Avaliação no conjunto de teste (para Early Stopping e Learning Rate Scheduler)\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch_val, y_batch_val in test_loader:\n",
    "                X_batch_val, y_batch_val = X_batch_val.to(device), y_batch_val.to(device)\n",
    "                outputs_val = model(X_batch_val)\n",
    "                loss_val = criterion(outputs_val, y_batch_val)\n",
    "                total_val_loss += loss_val.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "\n",
    "        train_losses_fold.append(avg_train_loss)\n",
    "        val_losses_fold.append(avg_val_loss)\n",
    "\n",
    "        # Step do scheduler com a perda de validação\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Lógica de Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_epoch_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"  Early Stopping at Epoch {epoch+1} for Fold {fold+1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "                model.load_state_dict(best_epoch_model_state)\n",
    "                break\n",
    "\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if (epoch + 1) % 20 == 0 or epoch == 0 or epoch == FNN_NUM_EPOCHS - 1 or epochs_no_improve == EARLY_STOPPING_PATIENCE:\n",
    "            print(f'  Epoch [{epoch+1}/{FNN_NUM_EPOCHS}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Current LR: {current_lr:.6f}')\n",
    "\n",
    "    plot_loss_curves(train_losses_fold, val_losses_fold, fold_num=fold+1, save_dir=RESULTS_OUTPUT_DIR, prefix=\"fnn_loss_curve_deeper_lr_sched\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    fold_predictions_scaled = []\n",
    "    fold_actual_scaled = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch_test_eval, y_batch_test_eval in test_loader:\n",
    "            X_batch_test_eval, y_batch_test_eval = X_batch_test_eval.to(device), y_batch_test_eval.to(device)\n",
    "            outputs_test_eval = model(X_batch_test_eval)\n",
    "            fold_predictions_scaled.extend(outputs_test_eval.cpu().numpy().flatten())\n",
    "            fold_actual_scaled.extend(y_batch_test_eval.cpu().numpy().flatten())\n",
    "\n",
    "    fold_predictions_scaled = np.array(fold_predictions_scaled)\n",
    "    fold_actual_scaled = np.array(fold_actual_scaled)\n",
    "\n",
    "    mse = mean_squared_error(fold_actual_scaled, fold_predictions_scaled)\n",
    "    mae = mean_absolute_error(fold_actual_scaled, fold_predictions_scaled)\n",
    "    r2 = r2_score(fold_actual_scaled, fold_predictions_scaled)\n",
    "\n",
    "    try:\n",
    "        pearson_corr, _ = pearsonr(fold_actual_scaled, fold_predictions_scaled)\n",
    "    except ValueError:\n",
    "        pearson_corr = np.nan\n",
    "\n",
    "    mse_scores_scaled.append(mse)\n",
    "    mae_scores_scaled.append(mae)\n",
    "    r2_scores_scaled.append(r2)\n",
    "    pearson_scores_scaled.append(pearson_corr)\n",
    "\n",
    "    print(f\"Fold {fold+1} Resultados (escala escalada): MSE={mse:.4f}, MAE={mae:.4f}, R2={r2:.4f}, Pearson={pearson_corr:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Resultados Finais da Validação Cruzada (Médias e Desvios Padrão) ---\")\n",
    "final_cv_results = {\n",
    "    \"MSE médio (escala escalada)\": f\"{np.nanmean(mse_scores_scaled):.4f} +/- {np.nanstd(mse_scores_scaled):.4f}\",\n",
    "    \"MAE médio (escala escalada)\": f\"{np.nanmean(mae_scores_scaled):.4f} +/- {np.nanstd(mae_scores_scaled):.4f}\",\n",
    "    \"R2 médio (escala escalada)\": f\"{np.nanmean(r2_scores_scaled):.4f} +/- {np.nanstd(r2_scores_scaled):.4f}\",\n",
    "    \"Correlação de Pearson média (escala escalada)\": f\"{np.nanmean(pearson_scores_scaled):.4f} +/- {np.nanstd(pearson_scores_scaled):.4f}\"\n",
    "}\n",
    "for metric, value in final_cv_results.items():\n",
    "    print(f\"{metric}: {value}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_j-SPLN6hVf",
    "outputId": "28215585-a8af-47c5-d9dc-620c12dad09a",
    "ExecuteTime": {
     "end_time": "2025-06-20T18:48:53.883487Z",
     "start_time": "2025-06-20T18:35:44.348963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Treino e Avaliação da FNN com Validação Cruzada ---\n",
      "\n",
      "--- Treinando FNN - Fold 1/5 ---\n",
      "  Epoch [1/250], Train Loss: 0.4093, Val Loss: 0.3938, Current LR: 0.001000\n",
      "  Epoch [20/250], Train Loss: 0.1424, Val Loss: 0.2205, Current LR: 0.001000\n",
      "  Epoch [40/250], Train Loss: 0.0941, Val Loss: 0.1880, Current LR: 0.001000\n",
      "  Epoch [60/250], Train Loss: 0.0574, Val Loss: 0.1923, Current LR: 0.000500\n",
      "  Epoch [80/250], Train Loss: 0.0399, Val Loss: 0.1819, Current LR: 0.000250\n",
      "  Epoch [100/250], Train Loss: 0.0315, Val Loss: 0.1813, Current LR: 0.000125\n",
      "  Epoch [120/250], Train Loss: 0.0281, Val Loss: 0.1820, Current LR: 0.000063\n",
      "  Early Stopping at Epoch 124 for Fold 1. Best Val Loss: 0.1812\n",
      "Curva de perda salva em: .\\fnn_loss_curve_deeper_lr_sched_fold_1_20250620_193827.png\n",
      "Fold 1 Resultados (escala escalada): MSE=0.1797, MAE=0.2809, R2=0.6416, Pearson=0.8032\n",
      "\n",
      "--- Treinando FNN - Fold 2/5 ---\n",
      "  Epoch [1/250], Train Loss: 0.3964, Val Loss: 0.3239, Current LR: 0.001000\n",
      "  Epoch [20/250], Train Loss: 0.1416, Val Loss: 0.2037, Current LR: 0.001000\n",
      "  Epoch [40/250], Train Loss: 0.0861, Val Loss: 0.1861, Current LR: 0.001000\n",
      "  Epoch [60/250], Train Loss: 0.0452, Val Loss: 0.1726, Current LR: 0.000250\n",
      "  Epoch [80/250], Train Loss: 0.0338, Val Loss: 0.1722, Current LR: 0.000125\n",
      "  Epoch [100/250], Train Loss: 0.0295, Val Loss: 0.1687, Current LR: 0.000031\n",
      "  Early Stopping at Epoch 102 for Fold 2. Best Val Loss: 0.1684\n",
      "Curva de perda salva em: .\\fnn_loss_curve_deeper_lr_sched_fold_2_20250620_194044.png\n",
      "Fold 2 Resultados (escala escalada): MSE=0.1655, MAE=0.2645, R2=0.6410, Pearson=0.8033\n",
      "\n",
      "--- Treinando FNN - Fold 3/5 ---\n",
      "  Epoch [1/250], Train Loss: 0.4013, Val Loss: 0.3404, Current LR: 0.001000\n",
      "  Epoch [20/250], Train Loss: 0.1391, Val Loss: 0.1892, Current LR: 0.001000\n",
      "  Epoch [40/250], Train Loss: 0.0913, Val Loss: 0.1864, Current LR: 0.001000\n",
      "  Epoch [60/250], Train Loss: 0.0570, Val Loss: 0.1826, Current LR: 0.000500\n",
      "  Epoch [80/250], Train Loss: 0.0381, Val Loss: 0.1784, Current LR: 0.000250\n",
      "  Early Stopping at Epoch 89 for Fold 3. Best Val Loss: 0.1749\n",
      "Curva de perda salva em: .\\fnn_loss_curve_deeper_lr_sched_fold_3_20250620_194242.png\n",
      "Fold 3 Resultados (escala escalada): MSE=0.1712, MAE=0.2722, R2=0.6470, Pearson=0.8074\n",
      "\n",
      "--- Treinando FNN - Fold 4/5 ---\n",
      "  Epoch [1/250], Train Loss: 0.3914, Val Loss: 0.3158, Current LR: 0.001000\n",
      "  Epoch [20/250], Train Loss: 0.1372, Val Loss: 0.1881, Current LR: 0.001000\n",
      "  Epoch [40/250], Train Loss: 0.0871, Val Loss: 0.1643, Current LR: 0.001000\n",
      "  Epoch [60/250], Train Loss: 0.0489, Val Loss: 0.1648, Current LR: 0.000500\n",
      "  Epoch [80/250], Train Loss: 0.0397, Val Loss: 0.1586, Current LR: 0.000250\n",
      "  Epoch [100/250], Train Loss: 0.0274, Val Loss: 0.1537, Current LR: 0.000125\n",
      "  Early Stopping at Epoch 113 for Fold 4. Best Val Loss: 0.1506\n",
      "Curva de perda salva em: .\\fnn_loss_curve_deeper_lr_sched_fold_4_20250620_194513.png\n",
      "Fold 4 Resultados (escala escalada): MSE=0.1513, MAE=0.2518, R2=0.6723, Pearson=0.8217\n",
      "\n",
      "--- Treinando FNN - Fold 5/5 ---\n",
      "  Epoch [1/250], Train Loss: 0.3993, Val Loss: 0.3327, Current LR: 0.001000\n",
      "  Epoch [20/250], Train Loss: 0.1391, Val Loss: 0.1882, Current LR: 0.001000\n",
      "  Epoch [40/250], Train Loss: 0.0898, Val Loss: 0.1711, Current LR: 0.001000\n",
      "  Epoch [60/250], Train Loss: 0.0543, Val Loss: 0.1638, Current LR: 0.000500\n",
      "  Epoch [80/250], Train Loss: 0.0393, Val Loss: 0.1601, Current LR: 0.000500\n",
      "  Epoch [100/250], Train Loss: 0.0292, Val Loss: 0.1599, Current LR: 0.000250\n",
      "  Epoch [120/250], Train Loss: 0.0276, Val Loss: 0.1563, Current LR: 0.000250\n",
      "  Epoch [140/250], Train Loss: 0.0228, Val Loss: 0.1550, Current LR: 0.000063\n",
      "  Early Stopping at Epoch 153 for Fold 5. Best Val Loss: 0.1527\n",
      "Curva de perda salva em: .\\fnn_loss_curve_deeper_lr_sched_fold_5_20250620_194853.png\n",
      "Fold 5 Resultados (escala escalada): MSE=0.1521, MAE=0.2575, R2=0.6757, Pearson=0.8232\n",
      "\n",
      "--- Resultados Finais da Validação Cruzada (Médias e Desvios Padrão) ---\n",
      "MSE médio (escala escalada): 0.1640 +/- 0.0110\n",
      "MAE médio (escala escalada): 0.2654 +/- 0.0103\n",
      "R2 médio (escala escalada): 0.6555 +/- 0.0153\n",
      "Correlação de Pearson média (escala escalada): 0.8118 +/- 0.0089\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "source": [
    "# --- 6. Avaliação Final no Dataset Completo ---\n",
    "print(\"\\n--- Métricas no Dataset Completo (com o modelo final treinado no dataset inteiro) ---\")\n",
    "\n",
    "input_dim = X_embeddings.shape[1]\n",
    "# ALTERADO: Passar ambas as dimensões ocultas para o construtor da FNN para o modelo final\n",
    "final_model = FNN(input_dim, FNN_HIDDEN_DIM_1, FNN_HIDDEN_DIM_2, dropout_rate=FNN_DROPOUT_RATE).to(device)\n",
    "final_criterion = nn.MSELoss()\n",
    "# ALTERADO: Adicionado weight_decay ao otimizador do modelo final\n",
    "final_optimizer = optim.Adam(final_model.parameters(), lr=FNN_LEARNING_RATE, weight_decay=FNN_WEIGHT_DECAY)\n",
    "\n",
    "# NOVO: Agendador de taxa de aprendizagem para o modelo final\n",
    "final_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    final_optimizer,\n",
    "    mode='min',\n",
    "    factor=LR_SCHEDULER_FACTOR,\n",
    "    patience=LR_SCHEDULER_PATIENCE,\n",
    "    min_lr=LR_SCHEDULER_MIN_LR,\n",
    ")\n",
    "\n",
    "full_dataset = TensorDataset(X_embeddings, y_scaled)\n",
    "full_loader = DataLoader(full_dataset, batch_size=FNN_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Treino do modelo final no dataset completo (com Early Stopping)\n",
    "best_val_loss_final_model = float('inf') # Reutiliza para monitorizar a perda no treino final\n",
    "epochs_no_improve_final_model = 0\n",
    "best_final_model_state = None\n",
    "\n",
    "train_losses_final_model = []\n",
    "\n",
    "for epoch in range(FNN_NUM_EPOCHS):\n",
    "    final_model.train()\n",
    "    total_train_loss_final_model = 0\n",
    "    for X_batch, y_batch in full_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_model(X_batch)\n",
    "        loss = final_criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        final_optimizer.step()\n",
    "        total_train_loss_final_model += loss.item()\n",
    "\n",
    "    avg_train_loss_final_model = total_train_loss_final_model / len(full_loader)\n",
    "    train_losses_final_model.append(avg_train_loss_final_model)\n",
    "\n",
    "    # Step do scheduler com a perda de treino (já que é o conjunto completo)\n",
    "    final_scheduler.step(avg_train_loss_final_model)\n",
    "\n",
    "    # Lógica de Early Stopping para o modelo final\n",
    "    if avg_train_loss_final_model < best_val_loss_final_model: # Monitoriza a perda de treino para ES\n",
    "        best_val_loss_final_model = avg_train_loss_final_model\n",
    "        epochs_no_improve_final_model = 0\n",
    "        best_final_model_state = final_model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve_final_model += 1\n",
    "        if epochs_no_improve_final_model >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"  Early Stopping for Final Model at Epoch {epoch+1}. Best Train Loss: {best_val_loss_final_model:.4f}\")\n",
    "            final_model.load_state_dict(best_final_model_state)\n",
    "            break\n",
    "\n",
    "    current_lr_final_model = final_optimizer.param_groups[0]['lr']\n",
    "    if (epoch + 1) % 20 == 0 or epoch == FNN_NUM_EPOCHS - 1 or epochs_no_improve_final_model == EARLY_STOPPING_PATIENCE:\n",
    "        print(f'  Final Model Epoch [{epoch+1}/{FNN_NUM_EPOCHS}], Train Loss: {avg_train_loss_final_model:.4f}, Current LR: {current_lr_final_model:.6f}')\n",
    "\n",
    "plot_loss_curves(train_losses_final_model, [], fold_num=None, title_suffix=\"Modelo Final (Treino Completo)\", save_dir=RESULTS_OUTPUT_DIR, prefix=\"fnn_final_model_loss_curve_deeper_lr_sched\")\n",
    "\n",
    "# Fazer previsões no dataset completo (na escala escalada)\n",
    "final_model.eval()\n",
    "final_predictions_scaled = []\n",
    "with torch.no_grad():\n",
    "    for X_batch_test, _ in DataLoader(TensorDataset(X_embeddings, y_scaled), batch_size=FNN_BATCH_SIZE, shuffle=False):\n",
    "        X_batch_test = X_batch_test.to(device)\n",
    "        outputs_test = final_model(X_batch_test)\n",
    "        final_predictions_scaled.extend(outputs_test.cpu().numpy().flatten())\n",
    "\n",
    "final_predictions_scaled = np.array(final_predictions_scaled)\n",
    "\n",
    "final_predictions_original = scaler.inverse_transform(final_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "global_mse = mean_squared_error(true_original_target_values, final_predictions_original)\n",
    "global_mae = mean_absolute_error(true_original_target_values, final_predictions_original)\n",
    "global_r2 = r2_score(true_original_target_values, final_predictions_original)\n",
    "try:\n",
    "    global_pearson, _ = pearsonr(true_original_target_values.flatten(), final_predictions_original.flatten())\n",
    "except ValueError:\n",
    "    global_pearson = np.nan\n",
    "\n",
    "# --- Cálculo das Percentagens de Erro ---\n",
    "global_rmse = np.sqrt(global_mse)\n",
    "\n",
    "mean_of_original_targets = np.mean(true_original_target_values)\n",
    "\n",
    "if mean_of_original_targets != 0:\n",
    "    pmae = (global_mae / mean_of_original_targets) * 100\n",
    "    prmse = (global_rmse / mean_of_original_targets) * 100\n",
    "else:\n",
    "    pmae = np.nan\n",
    "    prmse = np.nan\n",
    "\n",
    "global_results = {\n",
    "    \"MSE Global (escala original)\": f\"{global_mse:.4f}\",\n",
    "    \"MAE Global (escala original)\": f\"{global_mae:.4f}\",\n",
    "    \"MAE Percentual (vs. Média)\": f\"{pmae:.2f}%\",\n",
    "    \"RMSE Global (escala original)\": f\"{global_rmse:.4f}\",\n",
    "    \"RMSE Percentual (vs. Média)\": f\"{prmse:.2f}%\",\n",
    "    \"R2 Global (escala original)\": f\"{global_r2:.4f}\",\n",
    "    \"Pearson Global (escala original)\": f\"{global_pearson:.4f}\"\n",
    "}\n",
    "for metric, value in global_results.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "# --- Escala dos Valores Originais de Expressão Proteica ---\n",
    "min_original_prot = np.min(true_original_target_values)\n",
    "max_original_prot = np.max(true_original_target_values)\n",
    "std_original_prot = np.std(true_original_target_values)\n",
    "amplitude_original_prot = max_original_prot - min_original_prot\n",
    "\n",
    "print(f\"\\n--- Escala dos Valores Originais de Expressão Proteica ---\")\n",
    "print(f\"  Mínimo: {min_original_prot:.4f}\")\n",
    "print(f\"  Máximo: {max_original_prot:.4f}\")\n",
    "print(f\"  Média: {mean_of_original_targets:.4f}\")\n",
    "print(f\"  Desvio Padrão: {std_original_prot:.4f}\")\n",
    "print(f\"  Amplitude: {amplitude_original_prot:.4f}\")\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T18:58:49.548924Z",
     "start_time": "2025-06-20T18:48:54.140970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Métricas no Dataset Completo (com o modelo final treinado no dataset inteiro) ---\n",
      "  Final Model Epoch [20/250], Train Loss: 0.1322, Current LR: 0.001000\n",
      "  Final Model Epoch [40/250], Train Loss: 0.0901, Current LR: 0.001000\n",
      "  Final Model Epoch [60/250], Train Loss: 0.0671, Current LR: 0.001000\n",
      "  Final Model Epoch [80/250], Train Loss: 0.0574, Current LR: 0.001000\n",
      "  Final Model Epoch [100/250], Train Loss: 0.0480, Current LR: 0.001000\n",
      "  Final Model Epoch [120/250], Train Loss: 0.0484, Current LR: 0.001000\n",
      "  Final Model Epoch [140/250], Train Loss: 0.0440, Current LR: 0.000500\n",
      "  Final Model Epoch [160/250], Train Loss: 0.0288, Current LR: 0.000500\n",
      "  Final Model Epoch [180/250], Train Loss: 0.0227, Current LR: 0.000250\n",
      "  Final Model Epoch [200/250], Train Loss: 0.0222, Current LR: 0.000250\n",
      "  Final Model Epoch [220/250], Train Loss: 0.0199, Current LR: 0.000250\n",
      "  Final Model Epoch [240/250], Train Loss: 0.0214, Current LR: 0.000250\n",
      "  Final Model Epoch [250/250], Train Loss: 0.0195, Current LR: 0.000250\n",
      "Curva de perda salva em: .\\fnn_final_model_loss_curve_deeper_lr_sched_20250620_195847.png\n",
      "MSE Global (escala original): 67064764.2469\n",
      "MAE Global (escala original): 6162.5482\n",
      "MAE Percentual (vs. Média): 10.09%\n",
      "RMSE Global (escala original): 8189.3079\n",
      "RMSE Percentual (vs. Média): 13.41%\n",
      "R2 Global (escala original): 0.9840\n",
      "Pearson Global (escala original): 0.9928\n",
      "\n",
      "--- Escala dos Valores Originais de Expressão Proteica ---\n",
      "  Mínimo: 1357.1507\n",
      "  Máximo: 204059.9524\n",
      "  Média: 61077.1584\n",
      "  Desvio Padrão: 64770.3892\n",
      "  Amplitude: 202702.8018\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7P0fIuv95s0W",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "514930ff-b193-4534-be33-a58978fee4c3",
    "ExecuteTime": {
     "end_time": "2025-06-20T18:58:49.868003Z",
     "start_time": "2025-06-20T18:58:49.766350Z"
    }
   },
   "source": [
    "# --- 7. Guardar Resultados e Modelo ---\n",
    "import os\n",
    "\n",
    "# Criar a pasta de resultados se não existir\n",
    "results_folder = \"results_FNN_robust\"\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "    print(f\"Pasta '{results_folder}' criada com sucesso.\")\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Definir os caminhos dos ficheiros na pasta de resultados\n",
    "results_filename = os.path.join(results_folder, f\"{RESULTS_FILE_PREFIX}_{timestamp}.txt\")\n",
    "predictions_data_filename = os.path.join(results_folder, f\"{PREDICTIONS_DATA_PREFIX}_{timestamp}.npy\")\n",
    "model_save_filename = os.path.join(results_folder, f\"best_fnn_model_robust_{timestamp}.pth\")\n",
    "\n",
    "print(f\"\\n--- A guardar resultados em: {results_filename} ---\")\n",
    "\n",
    "with open(results_filename, 'w') as f:\n",
    "    f.write(f\"Resultados da Otimização da FNN com RobustScaler (Melhorada)\\n\")\n",
    "    f.write(f\"Data e Hora: {timestamp}\\n\")\n",
    "    f.write(f\"----------------------------------------------------\\n\\n\")\n",
    "    f.write(f\"Hiperparâmetros da FNN:\\n\")\n",
    "    f.write(f\"  Dimensão Oculta 1: {FNN_HIDDEN_DIM_1}\\n\")\n",
    "    f.write(f\"  Dimensão Oculta 2: {FNN_HIDDEN_DIM_2}\\n\")\n",
    "    f.write(f\"  Taxa de Aprendizagem: {FNN_LEARNING_RATE}\\n\")\n",
    "    f.write(f\"  Número de Épocas Máximo: {FNN_NUM_EPOCHS}\\n\")\n",
    "    f.write(f\"  Tamanho do Lote: {FNN_BATCH_SIZE}\\n\")\n",
    "    f.write(f\"  Taxa de Dropout: {FNN_DROPOUT_RATE}\\n\")\n",
    "    f.write(f\"  Weight Decay (L2 Reg.): {FNN_WEIGHT_DECAY}\\n\")\n",
    "    f.write(f\"  Patience para Early Stopping: {EARLY_STOPPING_PATIENCE}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Resultados da Validação Cruzada (Médias e Desvios Padrão - ESCALA ESCALADA (RobustScaler)):\\n\")\n",
    "    for metric, value in final_cv_results.items():\n",
    "        f.write(f\"  {metric}: {value}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Métricas no Dataset Completo (Com o modelo final treinado uma vez no dataset inteiro - ESCALA ORIGINAL):\\n\")\n",
    "    for metric, value in global_results.items():\n",
    "        f.write(f\"  {metric}: {value}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"--- Escala dos Valores Originais de Expressão Proteica ---\\n\")\n",
    "    f.write(f\"  Mínimo: {min_original_prot:.4f}\\n\")\n",
    "    f.write(f\"  Máximo: {max_original_prot:.4f}\\n\")\n",
    "    f.write(f\"  Média: {mean_of_original_targets:.4f}\\n\")\n",
    "    f.write(f\"  Desvio Padrão: {std_original_prot:.4f}\\n\")\n",
    "    f.write(f\"  Amplitude: {amplitude_original_prot:.4f}\\n\")\n",
    "    f.write(f\"\\n\")\n",
    "    f.write(f\"Informações Adicionais:\\n\")\n",
    "    f.write(f\"  Número de Amostras: {X_embeddings.shape[0]}\\n\")\n",
    "    f.write(f\"  Número de Features: {X_embeddings.shape[1]}\\n\")\n",
    "    f.write(f\"  Tipo de Embeddings: DNABERT\\n\")\n",
    "    f.write(f\"  Folds de Validação Cruzada: {n_splits}\\n\")\n",
    "    f.write(f\"  Ficheiro de Embeddings Usado: {os.path.basename(DNABERT_EMBEDDINGS_FILE)}\\n\")\n",
    "    f.write(f\"  Ficheiro de Modelo FNN Guardado: {os.path.basename(model_save_filename)}\\n\")\n",
    "    f.write(f\"  Ficheiro de Dados de Previsões Guardado: {os.path.basename(predictions_data_filename)}\\n\")\n",
    "    f.write(f\"  Ficheiro do Scaler Guardado: {os.path.basename(NORMALIZATION_SCALER_FILE)}\\n\")\n",
    "\n",
    "print(f\"Resultados guardados com sucesso em {results_filename}.\")\n",
    "\n",
    "# Guardar o modelo\n",
    "torch.save(final_model.state_dict(), model_save_filename)\n",
    "print(f\"Melhor modelo FNN guardado em: {model_save_filename}.\")\n",
    "\n",
    "# Guardar os dados de previsões\n",
    "data_for_plotting = np.vstack((true_original_target_values.flatten(), final_predictions_original)).T\n",
    "np.save(predictions_data_filename, data_for_plotting)\n",
    "print(f\"Valores reais (escala original) e previstos (escala original) guardados em {predictions_data_filename} para análise de gráficos.\")\n",
    "\n",
    "print(\"\\nScript de treino e avaliação da FNN concluído.\")\n",
    "print(f\"Todos os ficheiros foram guardados na pasta: {results_folder}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta 'results_FNN_robust' criada com sucesso.\n",
      "\n",
      "--- A guardar resultados em: results_FNN_robust\\fnn_optimization_results_20250620_195849.txt ---\n",
      "Resultados guardados com sucesso em results_FNN_robust\\fnn_optimization_results_20250620_195849.txt.\n",
      "Melhor modelo FNN guardado em: results_FNN_robust\\best_fnn_model_robust_20250620_195849.pth.\n",
      "Valores reais (escala original) e previstos (escala original) guardados em results_FNN_robust\\fnn_predictions_data_20250620_195849.npy para análise de gráficos.\n",
      "\n",
      "Script de treino e avaliação da FNN concluído.\n",
      "Todos os ficheiros foram guardados na pasta: results_FNN_robust\n"
     ]
    }
   ],
   "execution_count": 60
  }
 ]
}
